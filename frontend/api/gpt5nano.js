// /api/gpt5nano.js  (ESM)
import OpenAI from "openai";

export default async function handler(req, res) {
  try {
    if (req.method === "OPTIONS") return res.status(200).end();
    if (req.method !== "POST") {
      return res.status(405).json({ message: "Method Not Allowed", ok: false });
    }

    // Vercel Node í•¨ìˆ˜: ì•ˆì „í•˜ê²Œ ë°”ë”” ì§ì ‘ íŒŒì‹±
    let raw = "";
    for await (const chunk of req) raw += chunk;
    let data = {};
    try { data = JSON.parse(raw || "{}"); } catch { /* ignore */ }

    const prompt = data?.prompt;
    if (!prompt || typeof prompt !== "string") {
      return res.status(400).json({ message: "promptëŠ” ë¬¸ìì—´ì´ì–´ì•¼ í•©ë‹ˆë‹¤.", ok: false });
    }

    const hasKey = !!process.env.OPENAI_API_KEY;
    if (!hasKey) {
      return res.status(500).json({ message: "OPENAI_API_KEY ë¯¸ì„¤ì •", ok: false });
    }

    const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

    const r = await openai.chat.completions.create({
      model: "gpt-5-nano",
      messages: [
        { role: "system", content: "You are a helpful assistant." },
        { role: "user", content: prompt },
      ],
      reasoning_effort: "minimal",
      verbosity: "low",
    });

    const reply = r.choices?.[0]?.message?.content ?? "";
    return res.status(200).json({ ok: true, reply });
  } catch (err) {
    // ğŸ” ì—ëŸ¬ë¥¼ ê·¸ëŒ€ë¡œ ë…¸ì¶œ(ë¯¼ê°ì •ë³´ ì œì™¸) â€” ë””ë²„ê¹… í›„ì—” ë‹¤ì‹œ ìˆ¨ê²¨ë„ ë¨
    const status = err?.status || err?.response?.status || 500;
    const detail =
      err?.error?.message ||
      err?.response?.data?.error?.message ||
      err?.message ||
      "Unknown error";
    return res.status(status).json({
      ok: false,
      message: "Server error",
      detail,
      status,
      // stack: err?.stack, // í•„ìš”í•˜ë©´ ì„ì‹œë¡œ ì—´ê¸°
    });
  }
}
